{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Importing necessary modules and studying the dataset.**\n",
        "\n"
      ],
      "metadata": {
        "id": "gbzMPyqzu0Fw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQpAoeXPqZDQ",
        "outputId": "60fd58e5-e452-464b-f08c-2805f508e366"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Collecting conllu\n",
            "  Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-4.5.3\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install conllu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "metadata": {
        "id": "BhmeniYiqlBz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('conll2000')\n",
        "nltk.download('universal_tagset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoEUKHcVqk_c",
        "outputId": "c207a749-0194-41ef-e705-186a50ce7a73"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conlldata = list(nltk.corpus.conll2000.tagged_sents(tagset='universal'))"
      ],
      "metadata": {
        "id": "aVxTxP-Tqk9L"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(conlldata[5:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7Xd6GAjqk69",
        "outputId": "49b4be38-4ff8-4445-9685-0ea7aa081164"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('``', '.'), ('If', 'ADP'), ('there', 'DET'), ('is', 'VERB'), ('another', 'DET'), ('bad', 'ADJ'), ('trade', 'NOUN'), ('number', 'NOUN'), (',', '.'), ('there', 'DET'), ('could', 'VERB'), ('be', 'VERB'), ('an', 'DET'), ('awful', 'ADJ'), ('lot', 'NOUN'), ('of', 'ADP'), ('pressure', 'NOUN'), (',', '.'), (\"''\", '.'), ('noted', 'VERB'), ('Simon', 'NOUN'), ('Briscoe', 'NOUN'), (',', '.'), ('U.K.', 'NOUN'), ('economist', 'NOUN'), ('for', 'ADP'), ('Midland', 'NOUN'), ('Montagu', 'NOUN'), (',', '.'), ('a', 'DET'), ('unit', 'NOUN'), ('of', 'ADP'), ('Midland', 'NOUN'), ('Bank', 'NOUN'), ('PLC', 'NOUN'), ('.', '.')], [('Forecasts', 'NOUN'), ('for', 'ADP'), ('the', 'DET'), ('trade', 'NOUN'), ('figures', 'NOUN'), ('range', 'VERB'), ('widely', 'ADV'), (',', '.'), ('but', 'CONJ'), ('few', 'ADJ'), ('economists', 'NOUN'), ('expect', 'VERB'), ('the', 'DET'), ('data', 'NOUN'), ('to', 'PRT'), ('show', 'VERB'), ('a', 'DET'), ('very', 'ADV'), ('marked', 'VERB'), ('improvement', 'NOUN'), ('from', 'ADP'), ('the', 'DET'), ('#', '.'), ('2', 'NUM'), ('billion', 'NUM'), ('-LRB-', '.'), ('$', '.'), ('3.2', 'NUM'), ('billion', 'NUM'), ('-RRB-', '.'), ('deficit', 'NOUN'), ('in', 'ADP'), ('the', 'DET'), ('current', 'ADJ'), ('account', 'NOUN'), ('reported', 'VERB'), ('for', 'ADP'), ('August', 'NOUN'), ('.', '.')], [('The', 'DET'), ('August', 'NOUN'), ('deficit', 'NOUN'), ('and', 'CONJ'), ('the', 'DET'), ('#', '.'), ('2.2', 'NUM'), ('billion', 'NUM'), ('gap', 'NOUN'), ('registered', 'VERB'), ('in', 'ADP'), ('July', 'NOUN'), ('are', 'VERB'), ('topped', 'VERB'), ('only', 'ADV'), ('by', 'ADP'), ('the', 'DET'), ('#', '.'), ('2.3', 'NUM'), ('billion', 'NUM'), ('deficit', 'NOUN'), ('of', 'ADP'), ('October', 'NOUN'), ('1988', 'NUM'), ('.', '.')], [('Sanjay', 'NOUN'), ('Joshi', 'NOUN'), (',', '.'), ('European', 'ADJ'), ('economist', 'NOUN'), ('at', 'ADP'), ('Baring', 'NOUN'), ('Brothers', 'NOUN'), ('&', 'CONJ'), ('Co.', 'NOUN'), (',', '.'), ('said', 'VERB'), ('there', 'DET'), ('is', 'VERB'), ('no', 'DET'), ('sign', 'NOUN'), ('that', 'ADP'), ('Britain', 'NOUN'), (\"'s\", 'PRT'), ('manufacturing', 'NOUN'), ('industry', 'NOUN'), ('is', 'VERB'), ('transforming', 'VERB'), ('itself', 'PRON'), ('to', 'PRT'), ('boost', 'VERB'), ('exports', 'NOUN'), ('.', '.')], [('At', 'ADP'), ('the', 'DET'), ('same', 'ADJ'), ('time', 'NOUN'), (',', '.'), ('he', 'PRON'), ('remains', 'VERB'), ('fairly', 'ADV'), ('pessimistic', 'ADJ'), ('about', 'ADP'), ('the', 'DET'), ('outlook', 'NOUN'), ('for', 'ADP'), ('imports', 'NOUN'), (',', '.'), ('given', 'VERB'), ('continued', 'VERB'), ('high', 'ADJ'), ('consumer', 'NOUN'), ('and', 'CONJ'), ('capital', 'NOUN'), ('goods', 'NOUN'), ('inflows', 'NOUN'), ('.', '.')]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Function: Defining features for a sentence for extraction.**"
      ],
      "metadata": {
        "id": "5cVXeX99vHYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_function(sentence, i):\n",
        "  word = sentence[i][0]\n",
        "  pos = sentence[i][1]\n",
        "  features = {\n",
        "      'word': word,\n",
        "      'first_word': i == 0,\n",
        "      'last_word': i == len(sentence) - 1,\n",
        "      'prev_word': sentence[i-1][0],\n",
        "      'prev_pos': sentence[i-1][1],\n",
        "      'capitalized': word[1:].lower() != word[1:],\n",
        "      'prefix_1': word[:1],\n",
        "      'prefix_2': word[:2],\n",
        "      'prefix_3': word[:3],\n",
        "      'suffix_1': word[-1:],\n",
        "      'suffix_2': word[-2:],\n",
        "      'suffix_3': word[-3:]\n",
        "  }\n",
        "  return features"
      ],
      "metadata": {
        "id": "4q1yiDJmqk4M"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "3GnRvjCT2Ca-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train, data_test = train_test_split(conlldata, train_size=0.75,test_size=0.25)\n",
        "data_train, dev_set = train_test_split(conlldata, train_size=0.70, test_size=0.30)"
      ],
      "metadata": {
        "id": "etNKq01yqk1j"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_train[0][0:5])\n",
        "feature_function(data_train[0],0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2CQDeTy2crH",
        "outputId": "3376b128-974d-4382-952f-0db0a3e44305"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('A', 'DET'), ('Japan', 'NOUN'), ('Air', 'NOUN'), ('Lines', 'NOUN'), ('spokesman', 'NOUN')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'word': 'A',\n",
              " 'first_word': True,\n",
              " 'last_word': False,\n",
              " 'prev_word': '.',\n",
              " 'prev_pos': '.',\n",
              " 'capitalized': False,\n",
              " 'prefix_1': 'A',\n",
              " 'prefix_2': 'A',\n",
              " 'prefix_3': 'A',\n",
              " 'suffix_1': 'A',\n",
              " 'suffix_2': 'A',\n",
              " 'suffix_3': 'A'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn-crfsuite"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygtaPcSi3XFs",
        "outputId": "5ec7a8f7-afd7-441f-f0f6-ccc4cdf8bfef"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn-crfsuite\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Collecting python-crfsuite>=0.8.3 (from sklearn-crfsuite)\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite) (4.66.1)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.9 sklearn-crfsuite-0.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import metrics"
      ],
      "metadata": {
        "id": "A1KV4NuO3gNE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6BNivxhx4N37"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dc3fffJC4Nxz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementation of CRF**"
      ],
      "metadata": {
        "id": "lzq4uEfC9fFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearChainCRF:\n",
        "  # Assigning feature weights here\n",
        "  def __init__(self):\n",
        "    self.weights = None\n",
        "\n",
        "  # Defining feature function here\n",
        "  def feature_function(self, x, y_prev, y):\n",
        "    return features\n",
        "\n",
        "  # Computing transition score from y_prev to y given the token x\n",
        "  def transition(self, x, y_prev, y):\n",
        "    features = self.feature_function(x, y_prev, y)\n",
        "    return np.exp(np.dot(features, self.weights))\n",
        "\n",
        "  # Calculate marginal probabilites\n",
        "  # Also be used in prediction and parameter estimation\n",
        "  def forward_backward(self, x):\n",
        "    return forward_probs, backward_probs\n",
        "\n",
        "  # Finding the most likely sequence of labels\n",
        "  def viterbi(self, x):\n",
        "    return predicted_sequence\n",
        "\n",
        "  # Objective functin that will be used by optimizer\n",
        "  def gradient_optimizer(self, weights, *args):\n",
        "    return log_likelihood, grad\n",
        "\n",
        "  #Train the model\n",
        "  def fit(self, X_train, y_train):\n",
        "    global crf\n",
        "    crf = sklearn_crfsuite.CRF(\n",
        "        algorithm = 'lbfgs',\n",
        "        c1 = 0.1,\n",
        "        c2 = 0.1,\n",
        "        max_iterations = 100,\n",
        "        all_possible_transitions=True\n",
        "    )\n",
        "    crf.fit(X_train, y_train)\n",
        "\n",
        "  def predict(self, X_test):\n",
        "    return crf.predict(X_test)"
      ],
      "metadata": {
        "id": "p7jg3wNl2cot"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Usage**"
      ],
      "metadata": {
        "id": "yPrX-3QAT_3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linCRF = LinearChainCRF()\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "for sentence in conlldata:\n",
        "  X_sentence = []\n",
        "  y_sentence = []\n",
        "  for i in range(len(sentence)):\n",
        "    X_sentence.append(feature_function(sentence, i))\n",
        "    y_sentence.append(sentence[i][1])\n",
        "  X.append(X_sentence)\n",
        "  y.append(y_sentence)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
        "\n",
        "linCRF.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "tiaHtKgP2cl9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = linCRF.predict(X_test)"
      ],
      "metadata": {
        "id": "aBmyCYV-YOZu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.flat_accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLtHWoQrZVVG",
        "outputId": "e937cade-f2e4-44dc-b978-cb9aa4ec1ae8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9781790704129785\n"
          ]
        }
      ]
    }
  ]
}